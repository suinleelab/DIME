{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077880ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import feature_groups\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchmetrics import AUROC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from os import path\n",
    "from dime.data_utils import DenseDatasetSelected, get_group_matrix, get_xy, MaskLayerGrouped, data_split\n",
    "from dime.masking_pretrainer import MaskingPretrainer\n",
    "from dime.greedy_model_pl import GreedyCMIEstimatorPL\n",
    "from dime.utils import accuracy, auc, normalize, selection_without_cost\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11790a",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "intub_feature_names = feature_groups.intub_feature_names\n",
    "intub_feature_groups = feature_groups.intub_feature_groups\n",
    "device = torch.device('cuda', 1)\n",
    "\n",
    "cols_to_drop = []\n",
    "if cols_to_drop is not None:\n",
    "    intub_feature_names = [item for item in intub_feature_names if str(intub_feature_names.index(item)) not in cols_to_drop]\n",
    "\n",
    "# Load dataset\n",
    "dataset = DenseDatasetSelected('data/intub.csv', cols_to_drop=cols_to_drop)\n",
    "d_in = dataset.X.shape[1]  # 121\n",
    "d_out = len(np.unique(dataset.Y))  # 2\n",
    "feature_groups_dict, feature_groups_mask = get_group_matrix(intub_feature_names, intub_feature_groups)\n",
    "num_groups = len(feature_groups_mask) \n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(0))\n",
    "# Find mean/variance for normalizing\n",
    "x, y = get_xy(train_dataset)\n",
    "mean = np.mean(x, axis=0)\n",
    "std = np.std(y, axis=0)\n",
    "\n",
    "# Normalize via the original dataset\n",
    "dataset.X = dataset.X - mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9db45a",
   "metadata": {},
   "source": [
    "# Set up networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up architecture\n",
    "hidden = 128\n",
    "dropout = 0.3\n",
    "d_in = dataset.X.shape[1]  # 121\n",
    "d_out = len(np.unique(dataset.Y))  # 2\n",
    "print(d_out)\n",
    "# Outcome Predictor\n",
    "predictor = nn.Sequential(\n",
    "    nn.Linear(d_in + num_groups, hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(hidden, d_out)).to(device)\n",
    "\n",
    "# CMI Predictor\n",
    "value_network = nn.Sequential(\n",
    "    nn.Linear(d_in + num_groups, hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(hidden, num_groups),\n",
    "    nn.Sigmoid()).to(device)\n",
    "\n",
    "# Tie weights\n",
    "value_network[0] =  predictor[0]\n",
    "value_network[3] = predictor[3]\n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=128, shuffle=False, pin_memory=True,\n",
    "        drop_last=True, num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=128, shuffle=False, pin_memory=True,\n",
    "        drop_last=True, num_workers=4)\n",
    "\n",
    "\n",
    "mask_layer = MaskLayerGrouped(append=True, group_matrix=torch.tensor(feature_groups_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd03282",
   "metadata": {},
   "source": [
    "# Evaluate Penalized Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(5):\n",
    "    results_dict = {\"acc\": {}}\n",
    "    trained_model_path = f\"<path_to_trained_model>\"\n",
    "    greedy_cmi_estimator = GreedyCMIEstimatorPL.load_from_checkpoint(trained_model_path,\n",
    "                                                                     value_network=value_network,\n",
    "                                                                     predictor=predictor,\n",
    "                                                                     mask_layer=mask_layer,\n",
    "                                                                     lr=1e-3,\n",
    "                                                                    max_features=30,\n",
    "                                                                    eps=0.1,\n",
    "                                                                    loss_fn=nn.CrossEntropyLoss(reduction='none'),\n",
    "                                                                    val_loss_fn=auc,\n",
    "                                                                    eps_decay=True,\n",
    "                                                                    eps_decay_rate=0.2,\n",
    "                                                                    patience=5,\n",
    "                                                                    feature_costs=None,\n",
    "                                                                    use_entropy=True\n",
    "                                                            ).to(device)\n",
    "    avg_num_features_lambda = []\n",
    "    accuracy_scores_lambda = []\n",
    "    all_masks_lambda=[]\n",
    "    # Evaluation Mode lambda penalty\n",
    "    lamda_values = [0.000001, 0.00001, 0.00007, 0.0003, 0.0005] + [0.0012, 0.016] \n",
    "\n",
    "    for lamda in lamda_values:\n",
    "        metric_dict_lambda = greedy_cmi_estimator.evaluate(test_dataloader, performance_func=auc, \n",
    "                                                                        feature_costs=None, use_entropy=True, evaluation_mode='lamda-penalty', lamda=lamda)\n",
    "        accuracy_score = metric_dict_lambda['performance']\n",
    "        final_masks_lambda = metric_dict_lambda['final_masks']\n",
    "        accuracy_scores_lambda.append(accuracy_score)\n",
    "        avg_num_features_lambda.append(np.mean(np.sum(final_masks_lambda, axis=1)))\n",
    "        print(f\"Lambda={lamda}, AUC={accuracy_score}, Avg. num features={np.mean(np.sum(final_masks_lambda, axis=1))}\")\n",
    "        results_dict['acc'][np.mean(np.sum(final_masks_lambda, axis=1))] = accuracy_score\n",
    "\n",
    "        all_masks_lambda.append(final_masks_lambda)\n",
    "    \n",
    "    with open(f'results/intub_lamda_ours_trial_{trial}.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed40419",
   "metadata": {},
   "source": [
    "# Evaluate with Budget Constrained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ba0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(5):\n",
    "    results_dict = {\"acc\": {}}\n",
    "    trained_model_path = f\"<path_to_trained_model>\"\n",
    "    greedy_cmi_estimator = GreedyCMIEstimatorPL.load_from_checkpoint(trained_model_path,\n",
    "                                                                     value_network=value_network,\n",
    "                                                                     predictor=predictor,\n",
    "                                                                     mask_layer=mask_layer,\n",
    "                                                                     lr=1e-3,\n",
    "                                                                    max_features=40,\n",
    "                                                                    eps=0.05,\n",
    "                                                                    loss_fn=nn.CrossEntropyLoss(reduction='none'),\n",
    "                                                                    val_loss_fn=auc,\n",
    "                                                                    eps_decay=True,\n",
    "                                                                    eps_decay_rate=0.2,\n",
    "                                                                    patience=5,\n",
    "                                                                    feature_costs=None,\n",
    "                                                                    use_entropy=True\n",
    "                                                            ).to(device)\n",
    "    avg_num_features_budget = []\n",
    "    accuracy_scores_budget = []\n",
    "    all_masks_budget=[]\n",
    "    max_budget_values = [1, 3, 5, 10, 15, 20, 25]\n",
    "    for budget in max_budget_values:\n",
    "        metric_dict_budget  = greedy_cmi_estimator.evaluate(test_dataloader, performance_func=auc, \n",
    "                                                                        feature_costs=None, use_entropy=True, evaluation_mode='fixed-budget', budget=budget)#, selection_func=selection_without_cost)\n",
    "\n",
    "        accuracy_score = metric_dict_budget['performance']\n",
    "        final_masks_budget = metric_dict_budget['final_masks']\n",
    "        accuracy_scores_budget.append(accuracy_score)\n",
    "        avg_num_features_budget.append(np.mean(np.sum(final_masks_budget, axis=1)))\n",
    "        print(f\"Budget={budget}, AUC={accuracy_score}, Avg. num features={np.mean(np.sum(final_masks_budget, axis=1))}\")\n",
    "        results_dict['acc'][np.mean(np.sum(final_masks_budget, axis=1))] = accuracy_score\n",
    "        all_masks_budget.append(final_masks_budget)\n",
    "    \n",
    "    with open(f'results/intub_ours_trial_{trial}.pkl', 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4488e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adap_selec]",
   "language": "python",
   "name": "conda-env-adap_selec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
